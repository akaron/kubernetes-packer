# Purpose

Deploy a prometheus + grafana inside the k8s cluster, with TLS/HTTPS enabled.
In the end of this instruction, one can access prometheus in the browser of host machine
via `https://prometheus.localk8s:9091`.

The connection will look like:
```
host browser -> guest machine port 9091 -> nodePort of guest machine to k8s ingress -> app in k8s
```
Note that the port `9091` is defined in `../Vagrantfile`, can change it to others.

Here I use the [ingress-nginx-controller for baremetal +
nodePort](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service).
Usually the ingress-controller work with a Load Balancer (for instance, the
[aws NLB](https://kubernetes.github.io/ingress-nginx/deploy/#aws) or metalLB).
But this one there's no Load Balancer in front of k8s cluster. Instead, the
ingress-nginx-controller is in all the nodes and is exposed via the k8s
nodePort.

Also see the picture in https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#over-a-nodeport-service .


# Steps

In the following, except the `/etc/hosts` and the browser are related to the
`host machine`, all other operations are in the guest machine `master1`,
i.e., first need to

```
vagrant ssh master1
cd /vagrant/k8s
```


## Deploy the ingress-nginx-controller

Once the k8s cluster is ready, install the ingress-nginx controller via

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/baremetal/deploy.yaml
```

Then need to update a value by running this in command line:
```
kubectl edit svc ingress-nginx-controller -n ingress-nginx
```
and set the value of the `externalTrafficPolicy` field of the `ingress-nginx-controller` service spec to `Local`.

In production environment, one should use scripts like ansible to automate this step
(fetch the yaml, update the yaml, then `kubectl apply`).

## Prepare certificate

```
sh ./gen_cert.sh
```
It creates self-signed certificate and private key and put them to `kubectl secret`.

## Deploy prometheus and grafana
```
kubectl create -f /vagrant/k8s/prometheus-pvc.yml
helm install prometheus prometheus-community/prometheus -f /vagrant/k8s/prometheus-values.yml
kubectl create -f /vagrant/k8s/grafana-configmaps.yml
kubectl create -f /vagrant/k8s/grafana-deployment.yml
```

Wait a bit until these pods are ready.


## Forward the nodePort for ingress-nginx-controller
Then find out the port for https and forward it in the guest machine
```
export HTTPS_NODEPORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath="{.spec.ports[1].nodePort}")
socat TCP4-LISTEN:9091,fork TCP4:localhost:$HTTPS_NODEPORT &
```

Note:
* the cert name generated by `get_cert.sh` and deployed to k8s cluster is same as the
  one assigned in `prometheus-values.yml`: `server.ingress.tls['secretName']`
* again, port 9091 is defined in `../Vagrantfile`


## update /etc/hosts in host machine
The apps are served at `prometheus.localk8s` and `grafana.localk8s` (run `kubectl get
ingress` to find out). The `nodePort` of ingress-nginx-controller is exposed. But the host
cannot resolve it. We can simply add these names to `/etc/hosts` of host machine. The
additional lines may look like:

```
172.28.128.3  prometheus.localk8s
172.28.128.3  grafana.localk8s
```

To find the ip (`172.28.128.3` above) address of guest VM, in guest machine, one way is:
* find the network interface from `/etc/netplan/50-vagrant.yaml`, something like `enp0s8`
* then find the ip address of the interface, such as `ifconfig enp0s8|grep 'inet '`
* This may help to find the ip
```
ifconfig | grep -A5 $(grep -A1 ethernets /etc/netplan/50-vagrant.yaml |head -n2 | tail -n1)|grep 'inet '|awk '{print $2}'
```

## Done

Now in the browser of host machine, open https://prometheus.localk8s:9091 for prometheus
and https://grafana.localk8s:9091 for grafana.
Because it's self-signed certificate, there should have warning in browsers, which
is safe to ignore.

The default username/password for grafana is `admin/admin`.


# Other install options

## Without TLS
To run without TLS
* skip the step `Prepare certificate` above
* in **Forward the nodePort for ingress-nginx-controller**, in the jsonpath, use
  `{.spec.ports[0].nodePort}` instead
* disable TLS in `prometheus-values.yml` before run `helm install prometheus ...`

Finally, in browser of host machine, use `http://` instead of `http://`.


## Without ingress controller

TODO: verify the steps below

Disable TLS in `prometheus-values.yml` then
```
kubectl create -f /vagrant/k8s/prometheus-pv.yml
helm install prometheus stable/prometheus -f /vagrant/k8s/prometheus-values.yml
kubectl create -f /vagrant/k8s/grafana-configmaps.yml
kubectl create -f /vagrant/k8s/grafana-deployment.yml
export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
kubectl --namespace default port-forward $POD_NAME 9090 &
socat TCP4-LISTEN:9091,fork TCP4:localhost:9090 &
```
Then in the browser of host, open `http://localhost:9090` in browser to open prometheus.
For grafana, you can run the last two lines and use different pod and ports.

# Clean up
Just `kubectl delete -f xxx` could work.
Remember to remove the lines `prometheus.localk8s` and `grafana.localk8s` in the `/etc/hosts` of host machine.

Since `PersistentStorage` is on for prometheus-server, you can copy the data from `/data`
of the node `worker` and put it back in next install.  There are problems to use the
synced folder `/vagrant` (if shared by NFS, there's the file lock issue; if shared type is
`Virtualbox`, there's `mmap` issue)

# vagrant restart
If restart vagrant server, remember to run the **Forward the nodePort for
ingress-nginx-controller** step again.

# Notes
* In Grafana webui, if there's no CPU status, probably you forget to set the value of the
  `externalTrafficPolicy` field of the `ingress-nginx-controller` service to `Local`.
* The default values for prometheus deployment can be found in https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
* One can also turn off `PersistentStorage` in `prometheus-values.yml` and don't
  deploy the pv/pvc.  The persistent volume is in `/data/prometheus-data` of node
  `worker1`.
* In Grafana ui, if there's no CPU status, probably you forget to set the value of the
  `externalTrafficPolicy` field of the `ingress-nginx-controller` service to `Local`.
